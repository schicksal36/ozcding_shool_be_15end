import httpx
from bs4 import BeautifulSoup
from app.models.quote import Quote


# =====================================================================
# ğŸ”¥ scrape_and_save_quotes â€” saramro.com ëª…ì–¸ ìŠ¤í¬ë˜í•‘ í›„ DB ì €ì¥ í•¨ìˆ˜
# ---------------------------------------------------------------------
# FastAPIì˜ ë¹„ë™ê¸° í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°,
# httpx.AsyncClient + BeautifulSoup + Tortoise ORMì„ ì¡°í•©í•œ êµ¬ì¡°.
#
# ë™ì‘ ìš”ì•½:
#   1) saramro.com?page=N í˜•íƒœë¡œ í˜ì´ì§€ ë°˜ë³µ ì¡°íšŒ
#   2) HTML íŒŒì‹± â†’ ëª…ì–¸ í…ìŠ¤íŠ¸ & ì‘ê°€ ì¶”ì¶œ
#   3) DB ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ (content ê¸°ì¤€)
#   4) ìƒˆë¡œìš´ ëª…ì–¸ë§Œ DBì— ì €ì¥
#   5) ì´ ì €ì¥ ê°œìˆ˜, ëˆ„ì  quote ìˆ˜ ë°˜í™˜
#
# ë¹„ë™ê¸° êµ¬ì¡°ì´ë¯€ë¡œ ë©”ì¸ ì„œë²„ê°€ ë¸”ë¡œí‚¹ë˜ì§€ ì•Šì•„ ê³ ì„±ëŠ¥ ìœ ì§€ ê°€ëŠ¥.
# =====================================================================
async def scrape_and_save_quotes(pages: int = 5):
    base_url = "https://saramro.com/quotes"
    saved_count = 0   # ìƒˆë¡œ ì €ì¥ëœ ëª…ì–¸ì˜ ê°œìˆ˜

    # -----------------------------------------------------------------
    # ğŸ”¥ httpx.AsyncClient()
    # -----------------------------------------------------------------
    # ë¹„ë™ê¸° ìš”ì²­ì„ ì§€ì›í•˜ëŠ” HTTP í´ë¼ì´ì–¸íŠ¸
    # requestsì™€ ë‹¬ë¦¬ await ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ FastAPIì— ì í•©í•¨.
    # ì„¸ì…˜ì„ async contextë¡œ ì—´ì–´ ì—¬ëŸ¬ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬.
    # -----------------------------------------------------------------
    async with httpx.AsyncClient() as client:

        # ==============================================================
        # ğŸ”¥ 1) ìš”ì²­í•  í˜ì´ì§€ ë°˜ë³µ
        # ==============================================================
        for page in range(1, pages + 1):
            url = f"{base_url}?page={page}"

            try:
                # -----------------------------------------------------
                # ğŸ”¸ HTTP GET ìš”ì²­
                # -----------------------------------------------------
                response = await client.get(url)

                # ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                if response.status_code != 200:
                    continue

                # -----------------------------------------------------
                # ğŸ”¸ HTML íŒŒì‹± (BeautifulSoup)
                # -----------------------------------------------------
                soup = BeautifulSoup(response.text, "html.parser")

                # saramro.com ì˜ ëª…ì–¸ ë¦¬ìŠ¤íŠ¸ëŠ” table êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŒ
                quote_elements = soup.select("table tbody tr")

                # ======================================================
                # ğŸ”¥ 2) í…Œì´ë¸” ê° row(tr) ì—ì„œ ëª…ì–¸ ì¶”ì¶œ
                # ======================================================
                for el in quote_elements:

                    # td[colspan="5"] ì˜ì—­ì´ ì‹¤ì œ ëª…ì–¸ì´ ë“¤ì–´ìˆëŠ” ê³³
                    scraped = el.select_one("td[colspan='5']")
                    if not scraped:
                        continue  # ëª…ì–¸ì´ ì•„ë‹Œ rowëŠ” ê±´ë„ˆëœ€

                    # -------------------------------------------------
                    # "ëª…ì–¸ ë‚´ìš© - ì‘ê°€" í˜•íƒœë¥¼ ê¸°ì¤€ìœ¼ë¡œ split
                    # -------------------------------------------------
                    cont_and_auth = scraped.get_text(strip=True).split("-")

                    content = cont_and_auth[0]               # ëª…ì–¸ ë³¸ë¬¸
                    author = cont_and_auth[1][1:] if len(cont_and_auth) > 1 else None
                    # author[1:] â†’ ì•ì— ë¶™ì–´ìˆëŠ” ê³µë°± ë¬¸ì ì œê±°

                    # -------------------------------------------------
                    # ğŸ”¥ 3) DB ì¤‘ë³µ ê²€ì‚¬ (contentë¡œ ê²€ì¦)
                    # -------------------------------------------------
                    exists = await Quote.filter(content=content).exists()

                    if not exists:
                        # ìƒˆë¡œìš´ ëª…ì–¸ë§Œ DB ì €ì¥
                        await Quote.create(content=content, author=author)
                        saved_count += 1

            except Exception as e:
                # í˜ì´ì§€ ë‹¨ìœ„ ì—ëŸ¬ëŠ” ë‹¤ë¥¸ í˜ì´ì§€ ì§„í–‰ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ì²˜ë¦¬
                print(f"Error scraping page {page}: {e}")
                continue

    # ==============================================================
    # ğŸ”¥ 4) ì „ì²´ ëª…ì–¸ ê°œìˆ˜ ì¡°íšŒ í›„ ë°˜í™˜
    # ==============================================================
    total_count = await Quote.all().count()

    return {
        "message": f"Scraping completed. Saved {saved_count} new quotes.",
        "total_quotes": total_count,
    }


